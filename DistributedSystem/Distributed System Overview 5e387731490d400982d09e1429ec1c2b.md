# Distributed System Overview

Created: June 4, 2022 6:54 PM

# 分布式存储

- 整合大量机器，对外作为整体提供存储服务
- 分布式系统 特性，包括扩展、成本、性能、易用的考虑（我理解上就是性能＋容错）
- 分布式存储的主要topic：数据分布、一致性、容错、负载均衡、事务与并发控制、使用API、压缩
- 数据的需求：非结构化（文本、图片、音视频）、结构化（预先定义schema）、半结构化（HTML），根据数据需求主要有四类分布式存储系统
    - 分布式文件系统：Blob（binary large object）数据，包括存储文本、照片、视频等
        - 常作为 分布式数据库、分布式表格等的底层存储
        - 文件系统内部使用chunk来组织数据，每个chunk大小基本相同，可以包含多个blob对象或者定长块，一个大文件可以由多个chunk构成
        - 对blob、定长块、大文件的操作基于chunk来实现
    - 分布式kv系统，提供基于key的CRUD功能，存储引擎常使用LSM
    - 分布式表格系统：常用于存储半结构化数据，不仅支持CRUD功能，并且支持很多数据的单表功能，但并不支持复杂操作
    - 分布式数据库：从单机书库扩展，并且由SQL 和 NoSQL数据库们争雄
- 单机大致性能参数如下

![Untitled](Distributed%20System%20Overview%205e387731490d400982d09e1429ec1c2b/Untitled.png)

## 单机存储引擎

### Hash storage engine —— Bitcask

![Untitled](Distributed%20System%20Overview%205e387731490d400982d09e1429ec1c2b/Untitled%201.png)

- 哈希表的索引结构保存在内存中，每次查找key的时候，通过索引查找数据所在文件以及位置
- 写入时，将k-v record append到 active file 的末尾，然后更新内存哈希表索引
    - value的长度可以原大于key的长度，因此在内存中的索引，可以用较小的空间索引较大的文件
- 删除和更新通过将原来记录标记，然后将索引指向新的记录，或者删除索引，为了防止文件膨胀，磁盘文件需要定期GC，将相同key的记录只保留最新的数据
- 为了防止crash 导致内存索引丢失，定期将内存索引 hint file转储到磁盘。同时在文件合并的时候，产生新的数据文件时 也产生一个索引文件，索引文件和数据文件的key相同，不过value只有相对位置信息，因此每次构建哈希表时，直接使用索引文件即可

### B Tree  storage engine

![Untitled](Distributed%20System%20Overview%205e387731490d400982d09e1429ec1c2b/Untitled%202.png)

- B tree 与B+ tree的区别，B tree 不保存重复的索引key，并且在索引到key的位置记录数据所在位置，而B+ tree叶子节点保存索引key的数据位置，非叶子节点只保存索引信息，聚簇索引会在叶子节点直接存储record
- 修改和更新操作，首先写WAL，然后修改内存中的节点，内存节点中dirty page超过一定比例，后台线程负责flush 到磁盘
- buffer pool，一块用来内存直接缓存磁盘数据的空间，绕开OS的buffer cache，常用的evict policy :
    - LRU：将最长时间没有读写的page，如果dirty需要flush，否则直接覆写
    - LIRS：如果一次进行大量read操作，可能将整个buffer pool刷新一遍，如果使用LRU。因此最好将热点数据长久驻留内存，因此将buffer pool中的数据分区，新数据加入old sublist，热点数据使用new sublist，如果page在old sublist停留超过一定时间，移入new sublist

### LSM tree storage engine

![Untitled](Distributed%20System%20Overview%205e387731490d400982d09e1429ec1c2b/Untitled%203.png)

- 分为内存tree和磁盘tree，数据更新操作全在内存中执行，内存tree大小超过限制，整个批量刷入磁盘，读取时，优先从内存和磁盘缓存查，如果找不到，从磁盘读取历史数据
    - 优点在于规避HDD的随机写 并在 SSD中可以改善写放大的问题，因为批量写入导致的write操作减少
    - 典型代表LevelDB
- 磁盘文件为sstable，每个文件基于key排序，并且由后台线程负责磁盘文件的compaction

### 并发控制

- 数据库锁，多粒度的读写锁
    - 死锁的解决，预防（顺序上锁）与检测（超时回滚与依赖检测）
- COW，对于一个写操作，将路径上所有节点copy 一份，在copy节点上修改，并发的读事务可以读之前的节点，修改提交时原子地将指针改到新地节点
- MVCC，对每行数据维护多个版本，根据事务隔离级别来查看相应版本的数据

### 故障恢复

- 常用Redo（事务提交重做） 和 Undo（事务中止回滚）日志
    - 什么先写日志然后修改内存数据？确保数据的一致性，如果先修改内存数据，被读到然后crash，重启后其实数据丢失
    - 优化：Gourp commit，日志缓存后批量写入磁盘，牺牲部分事务延时提高系统吞吐量
    - Check Point，内存数据不可能一直膨胀，需要定期转储到磁盘，当checkpoint写入日志中，说明日志之前的操作已经落盘，故障后只需要从checkpoint开始重做。如何支持check point期间不停止写操作？在日志文件中记录start check，然后把新的log记录下来，因为redo log的幂等性，在crash之后重做一遍start check 到 end check之间的log

### 数据压缩

- 压缩算法的核心——找重复数据，列存储通过相同列的数据组织一起，这样不仅减少了大数据分析需要查询的数据量，并且提高了数据的压缩比
- 二进制编码要求 一个字符编码不能是另一个字符的前缀（否则会导致歧义）
    - Huffman编码为了找出一种编码方式，使得编码的长度最短
    - LZ系列压缩算法：动态创建字典，压缩时遇到之前出现过的字符串使用<匹配串的相对位置，匹配长度>
    - BMDiff 和 Zippy（Snappy），压缩比相对不高，但是处理效率高

## 分布式系统

- 系统异常，服务器宕机、网络异常、磁盘故障，超时等
- 分布式一致性
    - 客户端角度：强一致性（client发出write请求之后，之后的read请求都可以看到修改结果）、弱一致性（不保证可以read到最新值）、最终一致性（有不一致结果的窗口期）
        - 读写一致性【read your write】：clientA写入了值，保证A的read一定可以看到最新的修改，但其他用户不保证
        - 会话一致性【session】：client和系统交互的整个会话期间保证读写一致性
        - 单调读【monotonic read】：如果clientA已经读取了某对象的某个值，不会读到更早的值
        - 单调写【monotonic write】：clintA的写按照顺序完成，系统的多个部分按照相同的顺序完成
    - 分布式系统角度
        - 副本一致性：集群内的replicated log 保持一致
        - 更新顺序一致性：集群内的servers 是否按照相同的顺序执行操作
- 系统的衡量指标
    - 性能：吞吐量及相应时间，QPS（query per second）、TPS（tx per second），latency 使用tp99 tp90等表示百分比请求的最大演示
    - 可用性：系统异常可以提供正常服务的能力
    - 一致性，只要系统设计合理，强一致性对性能和可用性的影响不会太大
    - 可扩展性，提供扩展集群规模来提升 容量、计算量、性能
- 数据分布与数据迁移
    - 哈希分布，最常见的就是使用 hash(key) % N （N表示是服务器或者集群数量）
        - 但是，如果key是某个主键，散列之后，同一user id的数据被分散到多个集群；如果按照user id散列，容易数据倾斜，data skew，比如网红大v的访问量、数据量极大
            - 针对大用户的方案，要么手动标记、要么自动动态识别再拆分
            - 传统hash如果N发生变化，数据需要大规模迁移，几乎所有数据都得重新分布
        - 一致性哈希算法，在一个哈希环上（0-2^n），给每个节点分配token，然后分配数据插入时，先计算key的哈希值，然后顺时针放入第一个大于等于整个哈希值的token节点
            - 优点：节点插入和删除，只会影响相邻的节点
            - 问题：如果删除的节点数据量相当庞大。会导致需要迁移的数据量过大
            - 节点信息存储方案：1、server记录prev 和next，空间O（1）时间 O（n）2、使用大小为n的路由表，记录server编号＋2^(i-1)的节点信息，空间时间都O（logN）3、空间换时间，记录所有server的解析信息，总控节点的存储另外由维护
    - 顺序分布，哈希会破坏数据的有序性，支持的是随机读取，而顺序扫描性能差
        - 通常通过root表和meta表，前者维护meta表位置，后者维护user表的位置信息
    - 负载均衡
        - 控制节点将收集节点资源使用情况，统计负载
    - 复制，
        - 强一致性：要求主备之间保持一致，只有备机返回收到log同步，才能给client返回成功结果
        - 异步复制协议：主机收到client请求写入local log就返回，异步复制log。但可能丢失部分更新
- CAP理论：一致性、可用性、分区容错三者不可同时满足
    - C：read操作总可以read之前完成write的操作，相对于client
    - A：单机故障依然系统正常，不需要等待重启等
    - P：机器、网络、停电等情况下，可以满足一致性和可用性
    - Oracle的复制组件包含三种模式，最大保护（强同步）、最大性能（异步复制）、最大可用（折衷方案，当主备网络故障切换至最大性能）
- 容错，故障检测通过lease租约协议实现
    - 单机故障和地盘故障在数据中心基本每天都有，主机A向主机B发送lease，B只有在有效期内才可以提供服务，超时没有续租成功将主动停止服务。当B没有续租成功，A可以将B的服务安全迁移至其他服务器（需要考虑时间误差，添加提前量）
- 故障恢复
    - 单层结构，数据分片后的数据分别将副本复制，并在不同的节点选择主副本提供服务
        - 恢复时只需要切换主副本
    - 双层结构，所有数据持久化入分布式文件系统，每个数据分片同一时刻只有一个提供服务的节点
        - 恢复时从磁盘中建立内存数据或者索引
- 可扩展性：
    - 总控节点：用于维护数据分布、执行集群成员管理、数据定位、故障检测、恢复、负载均衡等全局调度事务
        - 优点：设计简单、容易实现强一致性、用户友好
        - 性能瓶颈？只有分布式文件系统在内存中的目录数结构，可能内存容量不够，如果真的容量不够，可以多设计一层DFS-metadata的servers，提供索引DFS工作机的服务
    - 数据库扩容，垂直拆分-业务功能，水平拆分-哈希等同一类数据
        - 传统数据的扩容挑战：扩容不灵活（通常直接双倍扩容）、扩容不自动化（大量数据迁移）、复制副本时间太长
        - 同构系统：同一组内的节点服务相同的数据（数据复制的时间太长）
        - 异构系统：将数据分片，并且每个分片的副本放置在不同的节点，一节点可能维护分片A、分片B、分片D的副本，当节点crash，整个集群参与到对这三个分片的复制

### 分布式协议

- 2PL，实现分布式事务，需要所有事务执行节点达成一致
    - 两种故障：事务参与者发生故障（设置超时时间）；协调者发生故障（使用备机，如果没有，事务将阻塞）
    - 协调者集群可以使用Paxos保证一个crash，及时有新的
- Paxos，由一个Proposer提议者，发出给所有节点发accept请求，当超过一半的acceptor接收，发送ack通知acceptor，提议生效
    - 提议可以是选举也可以是操作，
    - Prepare阶段，提议者选择一个序列号n，发给其他acceptor prepare消息，接收者查看后根据序列号n，如果大于已经回复的所有prepare消息，将上次接收的提议发送给提议者，并承诺不再回复小于n的消息
    - accept阶段，在大多数回复prepare消息后，进行accept，如果prepare阶段acceptor回复了上次的提议，选择最大序列号的提议发给acceptor批准；否则生成一个新的发出。acceptor在不违反承诺的前提下接受
    - ack阶段，如果超过一半acceptor接受，提议生效，提议者发送ack消息